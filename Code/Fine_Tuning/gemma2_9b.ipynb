{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GEMMA-2-9B-IT FINE-TUNING WITH LORA (CLEAN VERSION)\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "[STEP 2] Loading and preparing training data...\n",
      "Loaded training data: (1440, 15)\n",
      "Train samples: 1152\n",
      "Validation samples: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1152/1152 [00:00<00:00, 2914.66 examples/s]\n",
      "Map: 100%|██████████| 288/288 [00:00<00:00, 4409.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Applying LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 54,018,048 || all params: 9,295,724,032 || trainable%: 0.5811\n",
      "\n",
      "[STEP 4] Setting up training configuration...\n",
      "\n",
      "[STEP 5] Creating trainer...\n",
      "\n",
      "[STEP 6] Starting training...\n",
      "Training configuration:\n",
      "  - Model: google/gemma-2-9b-it\n",
      "  - Max epochs: 5\n",
      "  - Batch size: 2\n",
      "  - Learning rate: 0.0001\n",
      "  - Evaluation every: 50 steps\n",
      "  - Early stopping patience: 3\n",
      "  - LoRA rank: 16\n",
      "  - LoRA alpha: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/1440 21:29 < 22:47, 0.54 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.663870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>0.547292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.463329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.420515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.401714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.398796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.395086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.390140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.387888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.384921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.383551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.385187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.363100</td>\n",
       "      <td>0.384028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.384233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Saving model...\n",
      "Model saved to: /home/diya.thakor/AirQuality/BASELINE/finetuned/gemma-2-9b-it-pm25-clean\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GEMMA-2-9B-IT FINE-TUNING WITH LORA (CLEAN VERSION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Paths\n",
    "base_dir = Path(\"/home/diya.thakor/AirQuality/BASELINE\")\n",
    "data_dir = base_dir / \"data\"\n",
    "finetuned_dir = base_dir / \"finetuned\"\n",
    "finetuned_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Training configuration\n",
    "output_dir = finetuned_dir / \"gemma-2-9b-it-pm25-clean\"\n",
    "max_epochs = 5\n",
    "batch_size = 2\n",
    "learning_rate = 1e-4\n",
    "eval_steps = 50\n",
    "early_stopping_patience = 3\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model - NOW WORKS WITH YOUR TRANSFORMERS 4.57.0!\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Loading and preparing training data...\")\n",
    "\n",
    "# Load training data\n",
    "train_df = pd.read_csv(data_dir / \"train_2022.csv\")\n",
    "print(f\"Loaded training data: {train_df.shape}\")\n",
    "\n",
    "# Prompt templates (same as zero-shot)\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an air pollution assistant. \"\n",
    "    \"Strictly respond to queries with a single real number only. \"\n",
    "    \"Do not include any units, explanation, or punctuation. Just a single number.\"\n",
    ")\n",
    "# RQ1\n",
    "# USER_TEMPLATE = (\n",
    "#     \"What is the average PM2.5 concentration (in μg/m³) in {city}, {state} during {month}, {year}? \"\n",
    "#     \"Give a single number only.\"\n",
    "# )\n",
    "\n",
    "# RQ4\n",
    "USER_TEMPLATE = (\n",
    "    \"What is the average PM2.5 concentration (in μg/m³) in {city}, {state} during {month}, {year}? \"\n",
    "    \"Additional context: Temperature (AT) = {at}°C, NDVI = {ndvi}, Population = {pop}. \"\n",
    "    \"Give a single number only.\"\n",
    ")\n",
    "\n",
    "# lat,lon\n",
    "# USER_TEMPLATE = (\n",
    "#     \"What is the average PM2.5 concentration (in μg/m³) at location (latitude={lat}, longitude={lon}) during {month}, {year}? \"\n",
    "#     \"Give a single number only.\"\n",
    "# )\n",
    "\n",
    "# Convert month numbers to month names\n",
    "month_names = {\n",
    "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
    "    5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n",
    "    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "}\n",
    "\n",
    "def create_training_text(row):\n",
    "    \"\"\"Create training text in the format the model expects\"\"\"\n",
    "    month_name = month_names[row['month']]\n",
    "\n",
    "    # Format auxiliary features\n",
    "    at_value = f\"{row['AT']:.1f}\" if pd.notna(row['AT']) else \"N/A\"\n",
    "    ndvi_value = f\"{row['avg_ndvi']:.3f}\" if pd.notna(row['avg_ndvi']) else \"N/A\"\n",
    "    pop_value = f\"{int(row['population'])}\" if pd.notna(row['population']) else \"N/A\"\n",
    "    \n",
    "    # Format coordinates\n",
    "    lat_value = f\"{row['latitude']:.4f}\" if pd.notna(row['latitude']) else \"N/A\"\n",
    "    lon_value = f\"{row['longitude']:.4f}\" if pd.notna(row['longitude']) else \"N/A\"\n",
    "\n",
    "    \n",
    "    # Create the full conversation\n",
    "    # messages = [\n",
    "    #     {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(city=row['city'], state=row['state'], month=month_name, year=row['year'])}\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": str(row['PM2.5'])}\n",
    "    # ]\n",
    "    \n",
    "    # messages = [\n",
    "    #     {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(\n",
    "    #         city=row['city'], \n",
    "    #         state=row['state'], \n",
    "    #         month=month_name, \n",
    "    #         year=row['year'],\n",
    "    #         at=at_value,\n",
    "    #         ndvi=ndvi_value,\n",
    "    #         pop=pop_value\n",
    "    #     )}\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": str(row['PM2.5'])}\n",
    "    # ]\n",
    "\n",
    "    # Create the full conversation with lat/lon\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{USER_TEMPLATE.format(\n",
    "            lat=lat_value,\n",
    "            lon=lon_value,\n",
    "            month=month_name, \n",
    "            year=row['year']\n",
    "        )}\"},\n",
    "        {\"role\": \"assistant\", \"content\": str(row['PM2.5'])}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_token=False)\n",
    "    return text\n",
    "\n",
    "# Create training texts\n",
    "training_texts = [create_training_text(row) for _, row in train_df.iterrows()]\n",
    "\n",
    "# Split into train/validation (80/20)\n",
    "train_texts, val_texts = train_test_split(\n",
    "    training_texts, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts})\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY LORA\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Applying LoRA...\")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Setting up training configuration...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    num_train_epochs=max_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    local_rank=-1,\n",
    "    ddp_find_unused_parameters=False, \n",
    "    no_cuda=False,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=eval_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    logging_dir=str(output_dir / \"logs\"),\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    dataloader_drop_last=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=None\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Creating trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 6] Starting training...\")\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Model: {model_id}\")\n",
    "print(f\"  - Max epochs: {max_epochs}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Learning rate: {learning_rate}\")\n",
    "print(f\"  - Evaluation every: {eval_steps} steps\")\n",
    "print(f\"  - Early stopping patience: {early_stopping_patience}\")\n",
    "print(f\"  - LoRA rank: {lora_config.r}\")\n",
    "print(f\"  - LoRA alpha: {lora_config.lora_alpha}\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 7] Saving model...\")\n",
    "\n",
    "# Save the fine-tuned model (LoRA adapters)\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "import shutil\n",
    "for checkpoint_dir in output_dir.glob(\"checkpoint-*\"):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing on fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/diya_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GEMMA-2-9B-IT FINE-TUNED MODEL BATCH INFERENCE\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading base model and fine-tuned adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded on cuda:1\n",
      "\n",
      "[STEP 2] Loading test data...\n",
      "Loaded test data: (1932, 15)\n",
      "Starting fresh batch inference run\n",
      "Remaining samples to process: 1932\n",
      "\n",
      "[STEP 3] Running batch inference (batch_size=8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:   0%|          | 0/242 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Batch Processing:   4%|▍         | 10/242 [00:10<03:41,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 80 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:   8%|▊         | 20/242 [00:19<03:36,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 160 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  12%|█▏        | 30/242 [00:29<03:21,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 240 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  17%|█▋        | 40/242 [00:38<03:13,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 320 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  21%|██        | 50/242 [00:48<03:02,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 400 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  25%|██▍       | 60/242 [00:57<02:51,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 480 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  29%|██▉       | 70/242 [01:07<02:42,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 560 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  33%|███▎      | 80/242 [01:16<02:32,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 640 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  37%|███▋      | 90/242 [01:26<02:25,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 720 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  41%|████▏     | 100/242 [01:35<02:14,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 800 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  45%|████▌     | 110/242 [01:45<02:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 880 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  50%|████▉     | 120/242 [01:54<01:56,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 960 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  54%|█████▎    | 130/242 [02:04<01:46,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1040 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  58%|█████▊    | 140/242 [02:13<01:38,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1120 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  62%|██████▏   | 150/242 [02:23<01:28,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1200 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  66%|██████▌   | 160/242 [02:33<01:19,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1280 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  70%|███████   | 170/242 [02:42<01:09,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1360 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  74%|███████▍  | 180/242 [02:52<00:59,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1440 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  79%|███████▊  | 190/242 [03:02<00:50,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1520 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  83%|████████▎ | 200/242 [03:11<00:40,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1600 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  87%|████████▋ | 210/242 [03:21<00:31,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1680 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  91%|█████████ | 220/242 [03:31<00:21,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1760 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  95%|█████████▌| 230/242 [03:41<00:11,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1840 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:  99%|█████████▉| 240/242 [03:50<00:01,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: 1920 total results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing: 100%|██████████| 242/242 [03:52<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Saving final results...\n",
      "\n",
      "[STEP 5] Summary Statistics...\n",
      "Batch inference completed!\n",
      "Batch size used: 8\n",
      "Total samples processed: 1932\n",
      "New predictions made: 1932\n",
      "Error batches: 0\n",
      "Runtime: 0:03:52.437517\n",
      "Speed: 8.31 samples/second\n",
      "Results saved to: /home/diya.thakor/AirQuality/BASELINE/results/gemma2_finetuned_test_results.csv\n",
      "\n",
      "[STEP 6] Calculating Evaluation Metrics...\n",
      "Valid predictions: 1932 out of 1932\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "MAE (Mean Absolute Error):       18.43 μg/m³\n",
      "RMSE (Root Mean Squared Error):  26.82 μg/m³\n",
      "Spearman Correlation:             0.7445\n",
      "================================================================================\n",
      "\n",
      "Metrics saved to: /home/diya.thakor/AirQuality/BASELINE/results/gemma2_finetuned_test_results_metrics.txt\n",
      "\n",
      "[STEP 6] Cleanup...\n",
      "================================================================================\n",
      "BATCH INFERENCE COMPLETED SUCCESSFULLY\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "# Paths\n",
    "base_dir = \"/home/diya.thakor/AirQuality/BASELINE\"\n",
    "finetuned_model_path = f\"{base_dir}/finetuned/gemma-2-9b-it-pm25-clean\"\n",
    "test_data_path = f\"{base_dir}/data/test_2023.csv\"  \n",
    "results_path = f\"{base_dir}/results/gemma2_finetuned_test_results.csv\"\n",
    "\n",
    "# Model configuration\n",
    "base_model_id = \"google/gemma-2-9b-it\"\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# Batch processing configuration\n",
    "BATCH_SIZE = 8  \n",
    "MAX_NEW_TOKENS = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GEMMA-2-9B-IT FINE-TUNED MODEL BATCH INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS AND TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading base model and fine-tuned adapters...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map={\"\": 1},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    finetuned_model_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Fine-tuned model loaded on {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TEST DATA\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Loading test data...\")\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "print(f\"Loaded test data: {test_df.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT TEMPLATES\n",
    "# ============================================================================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an air pollution assistant. \"\n",
    "    \"Strictly respond to queries with a single real number only. \"\n",
    "    \"Do not include any units, explanation, or punctuation. Just a single number.\"\n",
    ")\n",
    "\n",
    "# RQ1\n",
    "\n",
    "# USER_TEMPLATE = (\n",
    "#     \"What is the average PM2.5 concentration (in μg/m³) in {city}, {state} during {month}, {year}? \"\n",
    "#     \"Give a single number only.\"\n",
    "# )\n",
    "\n",
    "#RQ4\n",
    "\n",
    "# USER_TEMPLATE = (\n",
    "#     \"What is the average PM2.5 concentration (in μg/m³) in {city}, {state} during {month}, {year}? \"\n",
    "#     \"Additional context: Temperature (AT) = {at}°C, NDVI = {ndvi}, Population = {pop}. \"\n",
    "#     \"Give a single number only.\"\n",
    "# )\n",
    "\n",
    "# With lat,lon\n",
    "\n",
    "USER_TEMPLATE = (\n",
    "    \"What is the average PM2.5 concentration (in μg/m³) at location (latitude={lat}, longitude={lon}) during {month}, {year}? \"\n",
    "    \"Give a single number only.\"\n",
    ")\n",
    "\n",
    "\n",
    "month_names = {\n",
    "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
    "    5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n",
    "    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH INFERENCE FUNCTIONS\n",
    "# ============================================================================\n",
    "def prepare_batch_prompts(batch_data):\n",
    "    \"\"\"Prepare batch of prompts for inference\"\"\"\n",
    "    prompts = []\n",
    "    for _, row in batch_data.iterrows():\n",
    "        city = row['city']\n",
    "        state = row['state']\n",
    "        month_name = month_names[row['month']]\n",
    "        year = row['year']\n",
    "\n",
    "        # Format auxiliary features (handle missing values)\n",
    "        at_value = f\"{row['AT']:.1f}\" if pd.notna(row['AT']) else \"N/A\"\n",
    "        ndvi_value = f\"{row['avg_ndvi']:.3f}\" if pd.notna(row['avg_ndvi']) else \"N/A\"\n",
    "        pop_value = f\"{int(row['population'])}\" if pd.notna(row['population']) else \"N/A\"\n",
    "\n",
    "        # Format coordinates\n",
    "        lat_value = f\"{row['latitude']:.4f}\" if pd.notna(row['latitude']) else \"N/A\"\n",
    "        lon_value = f\"{row['longitude']:.4f}\" if pd.notna(row['longitude']) else \"N/A\"\n",
    "    \n",
    "        \n",
    "        # user_prompt = USER_TEMPLATE.format(city=city, state=state, month=month_name, year=year)\n",
    "\n",
    "        # user_prompt = USER_TEMPLATE.format(\n",
    "        #     city=city, \n",
    "        #     state=state, \n",
    "        #     month=month_name, \n",
    "        #     year=year,\n",
    "        #     at=at_value,\n",
    "        #     ndvi=ndvi_value,\n",
    "        #     pop=pop_value\n",
    "        # )\n",
    "\n",
    "        user_prompt = USER_TEMPLATE.format(\n",
    "            lat=lat_value,\n",
    "            lon=lon_value,\n",
    "            month=month_name,\n",
    "            year=year\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\n{user_prompt}\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def batch_inference(prompts):\n",
    "    \"\"\"Run batch inference on list of prompts\"\"\"\n",
    "    # Tokenize all prompts at once\n",
    "    inputs = tokenizer(\n",
    "        prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate predictions for batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            top_k=1,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode batch outputs\n",
    "    predictions = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        # Extract only the generated part (after input)\n",
    "        input_length = inputs[\"input_ids\"][i].shape[0]\n",
    "        generated_tokens = output[input_length:]\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Extract number from response\n",
    "        match = re.search(r\"\\d+(\\.\\d+)?\", generated_text)\n",
    "        prediction = float(match.group()) if match else float(\"nan\")\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ============================================================================\n",
    "# RESUME FUNCTIONALITY (CLEAN VERSION)\n",
    "# ============================================================================\n",
    "if os.path.exists(results_path):\n",
    "    existing_results = pd.read_csv(results_path)\n",
    "    print(f\"Found existing results: {len(existing_results)} rows\")\n",
    "    processed_keys = set(zip(\n",
    "        existing_results['city'], \n",
    "        existing_results['state'], \n",
    "        existing_results['month'],\n",
    "        existing_results['year']\n",
    "    ))\n",
    "else:\n",
    "    print(\"Starting fresh batch inference run\")\n",
    "    existing_results = pd.DataFrame()\n",
    "    processed_keys = set()\n",
    "\n",
    "# Filter unprocessed data using actual data matching\n",
    "test_df['data_key'] = list(zip(test_df['city'], test_df['state'], test_df['month'], test_df['year']))\n",
    "unprocessed_df = test_df[~test_df['data_key'].isin(processed_keys)]\n",
    "print(f\"Remaining samples to process: {len(unprocessed_df)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "print(f\"\\n[STEP 3] Running batch inference (batch_size={BATCH_SIZE})...\")\n",
    "\n",
    "all_results = []\n",
    "error_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Process data in batches\n",
    "for start_idx in tqdm(range(0, len(unprocessed_df), BATCH_SIZE), desc=\"Batch Processing\"):\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(unprocessed_df))\n",
    "    batch_data = unprocessed_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        # Prepare batch prompts\n",
    "        prompts = prepare_batch_prompts(batch_data)\n",
    "        \n",
    "        # Run batch inference\n",
    "        predictions = batch_inference(prompts)\n",
    "        \n",
    "        # Store results\n",
    "        for i, (_, row) in enumerate(batch_data.iterrows()):\n",
    "            result = {\n",
    "                'city': row['city'],\n",
    "                'state': row['state'],\n",
    "                'month': row['month'],\n",
    "                'year': row['year'],\n",
    "                'pm2.5_predicted': predictions[i],\n",
    "                'pm2.5_actual': row['PM2.5'] \n",
    "            }\n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Save progress every 10 batches\n",
    "        if len(all_results) % (BATCH_SIZE * 10) == 0:\n",
    "            # Combine with existing results\n",
    "            if len(existing_results) > 0:\n",
    "                combined_results = pd.concat([existing_results, pd.DataFrame(all_results)], ignore_index=True)\n",
    "            else:\n",
    "                combined_results = pd.DataFrame(all_results)\n",
    "            \n",
    "            combined_results.to_csv(results_path, index=False)\n",
    "            print(f\"Saved checkpoint: {len(combined_results)} total results\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"Error processing batch {start_idx//BATCH_SIZE + 1}: {e}\")\n",
    "        \n",
    "        # Store error results for this batch\n",
    "        for _, row in batch_data.iterrows():\n",
    "            result = {\n",
    "                'city': row['city'],\n",
    "                'state': row['state'],\n",
    "                'month': row['month'],\n",
    "                'year': row['year'],\n",
    "                'pm2.5_predicted': float('nan'),\n",
    "                'pm2.5_actual': row['PM2.5']\n",
    "            }\n",
    "            all_results.append(result)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE FINAL RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Saving final results...\")\n",
    "\n",
    "if len(existing_results) > 0:\n",
    "    final_results = pd.concat([existing_results, pd.DataFrame(all_results)], ignore_index=True)\n",
    "else:\n",
    "    final_results = pd.DataFrame(all_results)\n",
    "\n",
    "final_results.to_csv(results_path, index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Summary Statistics...\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "runtime = end_time - start_time\n",
    "total_new_samples = len(all_results)\n",
    "\n",
    "print(f\"Batch inference completed!\")\n",
    "print(f\"Batch size used: {BATCH_SIZE}\")\n",
    "print(f\"Total samples processed: {len(final_results)}\")\n",
    "print(f\"New predictions made: {total_new_samples}\")\n",
    "print(f\"Error batches: {error_count}\")\n",
    "print(f\"Runtime: {runtime}\")\n",
    "print(f\"Speed: {total_new_samples / runtime.total_seconds():.2f} samples/second\")\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 6] Calculating Evaluation Metrics...\")\n",
    "\n",
    "# Remove rows with NaN predictions (errors)\n",
    "valid_results = final_results.dropna(subset=['pm2.5_predicted', 'pm2.5_actual'])\n",
    "print(f\"Valid predictions: {len(valid_results)} out of {len(final_results)}\")\n",
    "\n",
    "if len(valid_results) > 0:\n",
    "    y_true = valid_results['pm2.5_actual'].values\n",
    "    y_pred = valid_results['pm2.5_predicted'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"MAE (Mean Absolute Error):       {mae:.2f} μg/m³\")\n",
    "    print(f\"RMSE (Root Mean Squared Error):  {rmse:.2f} μg/m³\")\n",
    "    print(f\"Spearman Correlation:             {spearman_corr:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_dict = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'Spearman': spearman_corr,\n",
    "        'Valid_Predictions': len(valid_results),\n",
    "        'Total_Samples': len(final_results),\n",
    "        'Error_Rate': (len(final_results) - len(valid_results)) / len(final_results) * 100\n",
    "    }\n",
    "    \n",
    "    metrics_path = results_path.replace('.csv', '_metrics.txt')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"GEMMA-2-9B-IT FINE-TUNED MODEL - EVALUATION METRICS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Model: {base_model_id}\\n\")\n",
    "        f.write(f\"Fine-tuned checkpoint: {finetuned_model_path}\\n\")\n",
    "        f.write(f\"Test data: {test_data_path}\\n\")\n",
    "        f.write(f\"Inference date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(\"Metrics:\\n\")\n",
    "        f.write(f\"  MAE:       {mae:.2f} μg/m³\\n\")\n",
    "        f.write(f\"  RMSE:      {rmse:.2f} μg/m³\\n\")\n",
    "        f.write(f\"  Spearman:  {spearman_corr:.4f}\\n\\n\")\n",
    "        f.write(\"Data Statistics:\\n\")\n",
    "        f.write(f\"  Valid predictions:     {len(valid_results)}\\n\")\n",
    "        f.write(f\"  Total samples:         {len(final_results)}\\n\")\n",
    "        f.write(f\"  Error rate:            {metrics_dict['Error_Rate']:.2f}%\\n\")\n",
    "        f.write(f\"  Runtime:               {runtime}\\n\")\n",
    "        f.write(f\"  Processing speed:      {total_new_samples / runtime.total_seconds():.2f} samples/sec\\n\")\n",
    "    \n",
    "    print(f\"\\nMetrics saved to: {metrics_path}\")\n",
    "else:\n",
    "    print(\"No valid predictions found! Check error logs.\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 6] Cleanup...\")\n",
    "del model\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH INFERENCE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
